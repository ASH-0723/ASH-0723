{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Assignment Theory Questions"
      ],
      "metadata": {
        "id": "tWjjlVOafypR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1:  What is a Decision Tree, and how does it work in the context of classification ?\n",
        "\n",
        "# Answer:\n",
        "A **Decision Tree** is a supervised machine learning algorithm that is used for both **classification** and **regression** tasks. In the context of **classification**, a decision tree is used to assign labels to input data by learning decision rules from the features of the data.\n",
        "\n",
        "\n",
        "\n",
        "### **How It Works (Classification Context)**\n",
        "\n",
        "1. **Structure**:\n",
        "\n",
        "   * The tree is made up of **nodes**:\n",
        "\n",
        "     * **Root Node**: The first decision point based on the most significant feature.\n",
        "     * **Internal Nodes**: Each represents a decision based on a feature.\n",
        "     * **Leaf Nodes**: Final output or class labels (e.g., \"spam\" or \"not spam\").\n",
        "\n",
        "2. **Splitting**:\n",
        "\n",
        "   * At each node, the algorithm chooses a feature and a threshold (for numerical data) or category (for categorical data) to split the data.\n",
        "   * The goal is to **maximize the purity** of the resulting groups (i.e., make each group mostly belong to a single class).\n",
        "\n",
        "3. **Feature Selection Criteria**:\n",
        "\n",
        "   * Common metrics to decide splits:\n",
        "\n",
        "     * **Gini Impurity**\n",
        "     * **Entropy / Information Gain**\n",
        "     * **Gain Ratio**\n",
        "     * These measure how well a split separates the classes.\n",
        "\n",
        "4. **Prediction**:\n",
        "\n",
        "   * To classify a new instance, the model starts at the root and follows the path based on feature values until it reaches a leaf node, which gives the predicted class.\n",
        "\n",
        "\n",
        "\n",
        "### **Example**\n",
        "\n",
        "Suppose we are classifying emails as \"Spam\" or \"Not Spam\" based on features like:\n",
        "\n",
        "* Contains the word \"free\"\n",
        "* Has more than 100 words\n",
        "* Includes a link\n",
        "\n",
        "The decision tree might look like this:\n",
        "\n",
        "```\n",
        "              [Contains \"free\"?]\n",
        "                  /       \\\n",
        "              Yes           No\n",
        "            /                 \\\n",
        "     [Has link?]            Not Spam\n",
        "       /    \\\n",
        "     Yes     No\n",
        "   Spam   Not Spam\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "###  **Advantages**\n",
        "\n",
        "* Easy to understand and interpret\n",
        "* Requires little data preprocessing\n",
        "* Handles both numerical and categorical data\n",
        "\n",
        "### **Disadvantages**\n",
        "\n",
        "* Prone to overfitting\n",
        "* Can become complex if not pruned\n",
        "* Unstable to small data changes\n",
        "\n",
        "\n",
        "# Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree ?\n",
        "\n",
        "#Answer:\n",
        "In a **Decision Tree**, the goal at each step is to **split the dataset** in a way that best separates the classes. To do this, the tree uses **impurity measures** to evaluate how \"mixed\" or \"pure\" a set of labels is.\n",
        "\n",
        "Two of the most common impurity measures are:\n",
        "\n",
        "\n",
        "\n",
        "## 1. **Gini Impurity**\n",
        "\n",
        "### Definition:\n",
        "\n",
        "Gini Impurity measures the probability that a randomly chosen sample from the dataset would be **incorrectly classified** if it was randomly labeled according to the class distribution in the dataset.\n",
        "\n",
        "### Formula:\n",
        "\n",
        "For a dataset with classes $C_1, C_2, ..., C_k$:\n",
        "\n",
        "$$\n",
        "\\text{Gini}(D) = 1 - \\sum_{i=1}^{k} p_i^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $p_i$ is the probability (frequency) of class $i$ in dataset $D$\n",
        "\n",
        "### Interpretation:\n",
        "\n",
        "* Gini = 0 → Pure node (only one class)\n",
        "* Higher Gini = More mixed classes\n",
        "\n",
        "\n",
        "\n",
        "## 2. **Entropy (Information Gain)**\n",
        "\n",
        "###  Definition:\n",
        "\n",
        "Entropy measures the level of **disorder** or **uncertainty** in a dataset. The more mixed the class labels are, the higher the entropy.\n",
        "\n",
        "###  Formula:\n",
        "\n",
        "$$\n",
        "\\text{Entropy}(D) = - \\sum_{i=1}^{k} p_i \\log_2(p_i)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $p_i$ is the probability of class $i$ in the dataset $D$\n",
        "\n",
        "### Interpretation:\n",
        "\n",
        "* Entropy = 0 → Pure node\n",
        "* Maximum entropy → Classes are evenly mixed\n",
        "\n",
        "\n",
        "\n",
        "### **How They Impact Splits in a Decision Tree**\n",
        "\n",
        "###  Goal:\n",
        "\n",
        "The decision tree algorithm chooses the split that **minimizes impurity** (Gini or Entropy) in the child nodes after the split.\n",
        "\n",
        "### Process:\n",
        "\n",
        "1. For each possible feature and threshold:\n",
        "\n",
        "   * Calculate the impurity of the left and right child nodes after the split.\n",
        "   * Compute the **weighted average impurity** of these child nodes.\n",
        "2. Choose the split that **reduces impurity the most**.\n",
        "\n",
        "This reduction is:\n",
        "\n",
        "* **Gini Gain** = Parent Gini - Weighted Gini of children\n",
        "* **Information Gain** = Parent Entropy - Weighted Entropy of children\n",
        "\n",
        "\n",
        "\n",
        "## Example\n",
        "\n",
        "Suppose you have a dataset:\n",
        "\n",
        "| Class |\n",
        "| ----- |\n",
        "| Yes   |\n",
        "| Yes   |\n",
        "| No    |\n",
        "| No    |\n",
        "\n",
        "* Gini = $1 - (0.5^2 + 0.5^2) = 0.5$\n",
        "* Entropy = $- (0.5 \\log_2 0.5 + 0.5 \\log_2 0.5) = 1$\n",
        "\n",
        "If a split results in:\n",
        "\n",
        "* One child node with all \"Yes\" (pure → Gini = 0, Entropy = 0)\n",
        "* One with mixed classes\n",
        "\n",
        "The algorithm will favor this split because it **reduces impurity**.\n",
        "\n",
        "\n",
        "\n",
        "##  Gini vs Entropy: Which is Better?\n",
        "\n",
        "| Aspect            | Gini Impurity            | Entropy (Information Gain)                |\n",
        "| ----------------- | ------------------------ | ----------------------------------------- |\n",
        "| Speed             | Slightly faster (no log) | Slightly slower (uses log)                |\n",
        "| Splitting results | Often similar            | Often similar                             |\n",
        "| Tendency          | Gini is more “greedy”    | Entropy is more sensitive to distribution |\n",
        "\n",
        "Both work well in practice, and many libraries like **scikit-learn** use Gini by default.\n",
        "\n",
        "\n",
        "#Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "#Answer:\n",
        "\n",
        "##  **What is Pruning in Decision Trees?**\n",
        "\n",
        "**Pruning** is a technique used to reduce the size of a decision tree by removing sections that provide little or no predictive power. The goal is to prevent **overfitting** and improve the model's generalization on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "##  **Types of Pruning**\n",
        "\n",
        "###  **1. Pre-Pruning (Early Stopping)**\n",
        "\n",
        "**Definition:**\n",
        "Pre-pruning stops the tree **growth early**, before it becomes too complex.\n",
        "\n",
        "**How it works:**\n",
        "It uses conditions to **stop splitting** a node during the tree-building process. Examples of stopping criteria:\n",
        "\n",
        "* Maximum depth reached\n",
        "* Minimum number of samples in a node\n",
        "* Minimum information gain (or impurity reduction) from a split\n",
        "\n",
        "** Practical Advantage:**\n",
        " **Faster training** and reduced **computational cost**, especially on large datasets.\n",
        "\n",
        "\n",
        "\n",
        "###  **2. Post-Pruning (Reduced Error Pruning / Cost Complexity Pruning)**\n",
        "\n",
        "**Definition:**\n",
        "Post-pruning allows the tree to grow fully and then **removes unnecessary branches** after it's built.\n",
        "\n",
        "**How it works:**\n",
        "After building the full tree:\n",
        "\n",
        "* Evaluate the performance of each subtree on a **validation set**\n",
        "* Prune branches that do not improve (or hurt) validation accuracy\n",
        "* This simplifies the tree without much loss in performance\n",
        "\n",
        "** Practical Advantage:**\n",
        " **Improves generalization** by removing overfitted branches after seeing the full data structure.\n",
        "\n",
        "\n",
        "#Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "#Answer:\n",
        "##  **What is Information Gain?**\n",
        "\n",
        "**Information Gain (IG)** is a metric used in decision trees to **measure the effectiveness of an attribute (feature) in classifying the data**.\n",
        "\n",
        "It quantifies the **reduction in entropy** (i.e., disorder or impurity) achieved by splitting the dataset based on a specific feature.\n",
        "\n",
        "##  **Formula:**\n",
        "\n",
        "$$\n",
        "\\text{Information Gain} = \\text{Entropy(parent)} - \\sum_{i=1}^{k} \\frac{|D_i|}{|D|} \\cdot \\text{Entropy}(D_i)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $D$: Parent dataset\n",
        "* $D_i$: Subsets after splitting by a feature\n",
        "* $|D_i| / |D|$: Proportion of instances in subset $i$\n",
        "* Entropy is calculated as:\n",
        "\n",
        "  $$\n",
        "  \\text{Entropy}(D) = - \\sum_{j=1}^{c} p_j \\log_2(p_j)\n",
        "  $$\n",
        "\n",
        "  with $p_j$ being the probability of class $j$\n",
        "\n",
        "\n",
        "## **Why Is Information Gain Important?**\n",
        "\n",
        "In decision tree construction, **at each node**, the algorithm must decide **which feature to split on**.\n",
        "\n",
        "* **Information Gain tells us which feature gives the most reduction in uncertainty** about the target class.\n",
        "* The feature with the **highest Information Gain** is chosen as the **best split** at that node.\n",
        "\n",
        "\n",
        "\n",
        "## **Example:**\n",
        "\n",
        "Suppose you're classifying emails as \"Spam\" or \"Not Spam\" using the feature **\"Contains the word 'free'\"**.\n",
        "\n",
        "1. **Before the split** (parent node):\n",
        "\n",
        "   * 50% spam, 50% not spam → Entropy = 1 (maximum uncertainty)\n",
        "\n",
        "2. **After the split**:\n",
        "\n",
        "   * Group 1 (\"free\" = yes): 90% spam, 10% not spam → Low entropy\n",
        "   * Group 2 (\"free\" = no): 20% spam, 80% not spam → Low entropy\n",
        "\n",
        "3. **Information Gain**:\n",
        "\n",
        "   * High, because the split greatly reduces uncertainty\n",
        "\n",
        " So, the decision tree **selects this feature** to split the node.\n",
        "\n",
        "\n",
        "\n",
        "##  **Key Points**\n",
        "\n",
        "| Concept      | Description                                                        |\n",
        "| ------------ | ------------------------------------------------------------------ |\n",
        "| What it does | Measures how much \"information\" a feature gives us about the class |\n",
        "| Used for     | Selecting the **best feature** to split a node                     |\n",
        "| Goal         | Maximize Information Gain = Maximize purity after split            |\n",
        "| Result       | Smaller, more accurate trees that generalize better                |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "#Answer:\n",
        "\n",
        "##  **Real-World Applications of Decision Trees**\n",
        "\n",
        "Decision Trees are widely used across industries due to their simplicity and interpretability. Some common applications include:\n",
        "\n",
        "### 🔹 1. **Medical Diagnosis**\n",
        "\n",
        "* **Use case**: Classifying diseases based on symptoms, test results, and patient history.\n",
        "* **Example**: Predicting whether a tumor is benign or malignant based on features like size, shape, etc.\n",
        "\n",
        "### 🔹 2. **Credit Scoring & Risk Assessment**\n",
        "\n",
        "* **Use case**: Deciding whether to approve a loan or credit card application.\n",
        "* **Example**: Using features like income, debt, credit history, and employment status.\n",
        "\n",
        "### 🔹 3. **Fraud Detection**\n",
        "\n",
        "* **Use case**: Identifying fraudulent transactions.\n",
        "* **Example**: A decision tree might flag a transaction as suspicious if it happens in a different country or involves an unusually large amount.\n",
        "\n",
        "### 🔹 4. **Customer Churn Prediction**\n",
        "\n",
        "* **Use case**: Predicting if a customer is likely to leave a service (telecom, SaaS, etc.).\n",
        "* **Example**: Based on usage patterns, customer support calls, billing info.\n",
        "\n",
        "### 🔹 5. **Marketing and Targeted Advertising**\n",
        "\n",
        "* **Use case**: Segmenting customers and targeting offers based on behavior.\n",
        "* **Example**: Deciding which promotion to send to a user based on age, spending history, and activity.\n",
        "\n",
        "### 🔹 6. **Manufacturing and Quality Control**\n",
        "\n",
        "* **Use case**: Detecting defective products based on sensor data or measurements.\n",
        "\n",
        "\n",
        "\n",
        "## **Advantages of Decision Trees**\n",
        "\n",
        "| Advantage                    | Explanation                                                            |\n",
        "| ---------------------------- | ---------------------------------------------------------------------- |\n",
        "| ✔️ Easy to understand        | Tree-like structure is intuitive, even for non-technical stakeholders  |\n",
        "| ✔️ Requires little data prep | Handles both numerical and categorical data; no need for normalization |\n",
        "| ✔️ Non-linear relationships  | Can capture complex decision boundaries without transformations        |\n",
        "| ✔️ Fast inference            | Once built, predictions are fast and efficient                         |\n",
        "| ✔️ Works with missing values | Can be modified to handle missing data                                 |\n",
        "\n",
        "\n",
        "\n",
        "##  **Limitations of Decision Trees**\n",
        "\n",
        "| Limitation                                  | Explanation                                                                 |\n",
        "| ------------------------------------------- | --------------------------------------------------------------------------- |\n",
        "| ❌ **Overfitting**                           | Trees can grow too complex and memorize training data                       |\n",
        "| ❌ **Unstable**                              | Small changes in data can lead to very different tree structures            |\n",
        "| ❌ **Bias toward features with more levels** | Can favor features with many unique values                                  |\n",
        "| ❌ **Less accurate than ensemble methods**   | Alone, trees may underperform compared to models like Random Forests        |\n",
        "| ❌ **Hard to model smooth functions**        | Decision trees split data step-wise and struggle with gradual relationships |\n",
        "\n",
        "\n",
        "\n",
        "##  **Mitigation Tips**\n",
        "\n",
        "* Use **pruning** (pre or post) to prevent overfitting.\n",
        "* Use **ensemble methods** (like Random Forest or Gradient Boosting) to improve accuracy and stability.\n",
        "\n",
        "\n",
        "\n",
        "Let me know if you'd like examples or diagrams for any of these use cases!\n"
      ],
      "metadata": {
        "id": "K97_M9UUV68K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Assignment Practical Questions"
      ],
      "metadata": {
        "id": "jF7cGLSKf1aT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6:   Write a Python program to: ● Load the Iris Dataset ● Train a Decision Tree Classifier using the Gini criterion ● Print the model’s accuracy and feature importances.\n",
        "\n",
        "# Answer:\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the Iris Dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "feature_names = iris.feature_names\n",
        "target_names = iris.target_names\n",
        "\n",
        "# 2. Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Classifier using the Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 4. Make predictions\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# 5. Print the model's accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# 6. Print feature importances\n",
        "importances = clf.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance_df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FbNaIXicbju",
        "outputId": "0c17040b-2f36-4196-82df-957265919031"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Feature Importances:\n",
            "          Feature  Importance\n",
            "petal length (cm)    0.906143\n",
            " petal width (cm)    0.077186\n",
            " sepal width (cm)    0.016670\n",
            "sepal length (cm)    0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7:  Write a Python program to: ● Load the Iris Dataset ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree.\n",
        "\n",
        "# Answer:\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Classifier with max_depth=3\n",
        "clf_limited = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=42)\n",
        "clf_limited.fit(X_train, y_train)\n",
        "y_pred_limited = clf_limited.predict(X_test)\n",
        "accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "# 4. Train a fully-grown Decision Tree (no max_depth limit)\n",
        "clf_full = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf_full.fit(X_train, y_train)\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "# 5. Print the accuracies for comparison\n",
        "print(f\"Accuracy with max_depth=3: {accuracy_limited:.2f}\")\n",
        "print(f\"Accuracy with fully-grown tree: {accuracy_full:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "am8jH5cedCYM",
        "outputId": "5302f07e-57ce-4f57-ae82-020b87a9790b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with max_depth=3: 1.00\n",
            "Accuracy with fully-grown tree: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to: ● Load the California Housing dataset from sklearn ● Train a Decision Tree Regressor ● Print the Mean Squared Error (MSE) and feature importances\n",
        "\n",
        "# Answer:\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Split the dataset into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# 5. Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
        "\n",
        "# 6. Print feature importances\n",
        "importances = regressor.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(importance_df.to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "457--g99dcQE",
        "outputId": "ab39c6ba-4a35-489e-fcad-000251c0e639"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE): 0.4952\n",
            "\n",
            "Feature Importances:\n",
            "   Feature  Importance\n",
            "    MedInc    0.528509\n",
            "  AveOccup    0.130838\n",
            "  Latitude    0.093717\n",
            " Longitude    0.082902\n",
            "  AveRooms    0.052975\n",
            "  HouseAge    0.051884\n",
            "Population    0.030516\n",
            " AveBedrms    0.028660\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to: ● Load the Iris Dataset ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV ● Print the best parameters and the resulting model accuracy\n",
        "\n",
        "# Answer:\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define the parameter grid for tuning\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, None],\n",
        "    'min_samples_split': [2, 4, 6, 8, 10]\n",
        "}\n",
        "\n",
        "# 4. Initialize the Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# 5. Set up GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# 6. Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 7. Predict on the test set using the best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# 8. Print best parameters and test accuracy\n",
        "print(\"Best Parameters Found:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nModel Accuracy with Best Parameters: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4J5780cdtyM",
        "outputId": "e79b1aab-f00f-4987-ef76-ef4e851dd9e1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters Found:\n",
            "{'max_depth': 4, 'min_samples_split': 2}\n",
            "\n",
            "Model Accuracy with Best Parameters: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Imagine you’re working as a data scientist for a healthcare company that wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values. Explain the step-by-step process you would follow to: ● Handle the missing values ● Encode the categorical features ● Train a Decision Tree model ● Tune its hyperparameters ● Evaluate its performance And describe what business value this model could provide in the real-world setting.\n",
        "\n",
        "# Answer:\n",
        "\n",
        "## **Step 1: Handle Missing Values**\n",
        "\n",
        "* **Identify missing data**: Explore the dataset to understand which features have missing values and how much is missing.\n",
        "* **Decide on imputation strategy** based on feature type and missingness:\n",
        "\n",
        "  * For **numerical features**: Use strategies like mean, median, or more advanced imputation methods (e.g., K-Nearest Neighbors Imputer).\n",
        "  * For **categorical features**: Impute with the most frequent category or create a separate category like “Unknown.”\n",
        "* Optionally, **flag missing values** with new binary features indicating “missingness” if missingness itself is informative.\n",
        "\n",
        "\n",
        "## **Step 2: Encode Categorical Features**\n",
        "\n",
        "* Use **encoding techniques** suitable for Decision Trees (which can handle categorical variables but sklearn's implementation requires numeric inputs):\n",
        "\n",
        "  * **Label Encoding** for ordinal categorical features.\n",
        "  * **One-Hot Encoding** for nominal categorical features with a small number of categories.\n",
        "* Avoid high-cardinality one-hot encoding; consider **target encoding** or **frequency encoding** if categories are numerous.\n",
        "* Make sure encoding is consistent between training and test datasets.\n",
        "\n",
        "\n",
        "## **Step 3: Train a Decision Tree Model**\n",
        "\n",
        "* Split data into **training** and **validation/test** sets.\n",
        "* Initialize the Decision Tree Classifier, e.g., with default parameters initially.\n",
        "* Fit the model on the processed training data.\n",
        "* Monitor training to check for overfitting (Decision Trees can easily overfit without constraints).\n",
        "\n",
        "\n",
        "## **Step 4: Tune Hyperparameters**\n",
        "\n",
        "* Use **hyperparameter tuning** (e.g., GridSearchCV or RandomizedSearchCV) to optimize:\n",
        "\n",
        "  * `max_depth` (controls tree complexity)\n",
        "  * `min_samples_split` and `min_samples_leaf` (control minimum data to split/leaf)\n",
        "  * `criterion` (e.g., Gini vs. Entropy)\n",
        "  * Others like `max_features`, `max_leaf_nodes`\n",
        "* Use **cross-validation** during tuning to ensure robust parameter selection.\n",
        "\n",
        "\n",
        "## **Step 5: Evaluate Performance**\n",
        "\n",
        "* Evaluate the tuned model on a **held-out test set**.\n",
        "* Use relevant metrics for classification in healthcare:\n",
        "\n",
        "  * **Accuracy**: Overall correctness.\n",
        "  * **Precision and Recall**: Especially recall (sensitivity) to minimize false negatives (missing disease cases).\n",
        "  * **F1-Score**: Balance between precision and recall.\n",
        "  * **ROC-AUC**: How well the model separates classes.\n",
        "* Consider **confusion matrix** to understand error types.\n",
        "* If the dataset is imbalanced, consider techniques like **SMOTE** or adjusting decision thresholds.\n",
        "\n",
        "\n",
        "## **Business Value of the Model**\n",
        "\n",
        "* **Early detection and intervention**: Predicting disease presence early enables timely treatment, potentially saving lives and reducing healthcare costs.\n",
        "* **Resource optimization**: Helps prioritize patients for further testing or specialist referral, optimizing limited medical resources.\n",
        "* **Improved patient outcomes**: Proactive care based on prediction can improve prognosis and quality of life.\n",
        "* **Cost savings**: Prevents unnecessary tests or late-stage treatments by focusing attention where it’s most needed.\n",
        "* **Data-driven decision making**: Provides clinicians with actionable insights to support diagnosis, supplementing human expertise.\n",
        "\n"
      ],
      "metadata": {
        "id": "Wp4D9ItCeKh9"
      }
    }
  ]
}
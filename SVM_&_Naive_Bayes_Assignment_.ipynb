{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM & Naive Bayes Assignment Theory Questions"
      ],
      "metadata": {
        "id": "bAcd-7Yo2JgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1:  What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "# Answer:\n",
        "\n",
        "\n",
        "### üîπ What is SVM?\n",
        "\n",
        "SVM finds the **optimal hyperplane** that best separates data points of different classes in a high-dimensional space. The goal is to **maximize the margin** between the two classes ‚Äî the distance between the hyperplane and the nearest data points from each class.\n",
        "\n",
        "\n",
        "\n",
        "### üîπ How SVM Works:\n",
        "\n",
        "1. **Hyperplane**:\n",
        "\n",
        "   * In a 2D space, it's a line.\n",
        "   * In a 3D space, it's a plane.\n",
        "   * In higher dimensions, it's called a hyperplane.\n",
        "   * The best hyperplane is the one that **maximizes the margin** between classes.\n",
        "\n",
        "2. **Support Vectors**:\n",
        "\n",
        "   * These are the **data points closest to the hyperplane**.\n",
        "   * They are critical in defining the position and orientation of the hyperplane.\n",
        "   * Removing them would change the decision boundary.\n",
        "\n",
        "3. **Margin**:\n",
        "\n",
        "   * The margin is the gap between the support vectors of the two classes.\n",
        "   * SVM tries to **maximize this margin** to improve generalization.\n",
        "\n",
        "\n",
        "\n",
        "### üîπ SVM in Non-Linearly Separable Data:\n",
        "\n",
        "When data is not linearly separable, SVM uses two techniques:\n",
        "\n",
        "1. **Kernel Trick**:\n",
        "\n",
        "   * Maps the data to a higher-dimensional space where it **becomes linearly separable**.\n",
        "   * Common kernels:\n",
        "\n",
        "     * Linear\n",
        "     * Polynomial\n",
        "     * Radial Basis Function (RBF)\n",
        "     * Sigmoid\n",
        "\n",
        "2. **Soft Margin (C parameter)**:\n",
        "\n",
        "   * Allows some misclassifications to avoid overfitting.\n",
        "   * **C** is a regularization parameter:\n",
        "\n",
        "     * High C ‚Üí low tolerance for misclassification (less regularization).\n",
        "     * Low C ‚Üí more tolerance for misclassification (more regularization).\n",
        "\n",
        "# Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "# Answer:\n",
        "\n",
        "###  1. **Hard Margin SVM**\n",
        "\n",
        "* **Definition**: A **Hard Margin SVM** assumes that the data is **perfectly linearly separable** ‚Äî i.e., you can draw a straight line (or hyperplane) that completely separates the two classes **without any errors**.\n",
        "\n",
        "* **Characteristics**:\n",
        "\n",
        "  * No misclassifications are allowed.\n",
        "  * All data points must lie **outside or on** the margin boundaries.\n",
        "  * Maximizes the margin **under a strict constraint**.\n",
        "\n",
        "* **When to Use**:\n",
        "\n",
        "  * Only when your data is **perfectly separable**.\n",
        "  * Rare in real-world applications due to noise/outliers.\n",
        "\n",
        "* **Drawbacks**:\n",
        "\n",
        "  * **Very sensitive** to outliers and noise.\n",
        "  * Not flexible ‚Äî real-world data is rarely perfectly separable.\n",
        "\n",
        "\n",
        "\n",
        "###  2. **Soft Margin SVM**\n",
        "\n",
        "* **Definition**: A **Soft Margin SVM** allows some data points to **violate** the margin boundaries (i.e., be misclassified or fall inside the margin) in order to improve generalization.\n",
        "\n",
        "* **Characteristics**:\n",
        "\n",
        "  * Introduces a **tuning parameter** `C` (regularization parameter).\n",
        "\n",
        "    * Low `C`: wider margin, allows more misclassifications (better generalization).\n",
        "    * High `C`: narrower margin, fewer misclassifications (may overfit).\n",
        "  * Balances between **maximizing margin** and **minimizing classification error**.\n",
        "\n",
        "* **When to Use**:\n",
        "\n",
        "  * When data is **not perfectly separable** (which is common).\n",
        "  * Suitable for noisy, real-world data.\n",
        "\n",
        "* **Advantages**:\n",
        "\n",
        "  * More **robust** to outliers.\n",
        "  * Provides a **trade-off** between bias and variance.\n",
        "\n",
        "\n",
        "# Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "# Answer:\n",
        "### What is the Kernel Trick?\n",
        "\n",
        "* Instead of explicitly transforming data to a higher-dimensional space (which can be computationally expensive), the kernel trick **computes the dot product of the data in that higher-dimensional space without ever transforming the data explicitly**.\n",
        "* This allows SVM to create **non-linear decision boundaries** efficiently.\n",
        "\n",
        "\n",
        "\n",
        "### Why Use It?\n",
        "\n",
        "* Many datasets cannot be separated by a straight line (i.e., not linearly separable).\n",
        "* The kernel trick enables SVM to **learn complex, non-linear patterns** by mapping input features into **higher-dimensional feature spaces**, where a linear separator **can exist**.\n",
        "\n",
        "\n",
        "\n",
        "### Common Kernels in SVM:\n",
        "\n",
        "| Kernel Name        | Use Case                                               |\n",
        "| ------------------ | ------------------------------------------------------ |\n",
        "| **Linear**         | When data is linearly separable                        |\n",
        "| **Polynomial**     | When data shows polynomial relationships               |\n",
        "| **RBF (Gaussian)** | When data has circular or radial patterns (non-linear) |\n",
        "| **Sigmoid**        | Similar to neural networks (rarely used in practice)   |\n",
        "\n",
        "\n",
        "\n",
        "### Example: **Radial Basis Function (RBF) Kernel**\n",
        "\n",
        "#### Formula:\n",
        "\n",
        "$$\n",
        "K(x, x') = \\exp\\left(-\\gamma \\|x - x'\\|^2\\right)\n",
        "$$\n",
        "\n",
        "* Where:\n",
        "\n",
        "  * $x, x'$ are data points\n",
        "  * $\\gamma$ is a parameter that controls the spread of the kernel\n",
        "\n",
        "#### üîπ Use Case:\n",
        "\n",
        "* **Non-linearly separable data**, especially when the class boundaries are **circular or irregular**.\n",
        "* Very common in **image classification**, **bioinformatics**, and **anomaly detection**.\n",
        "\n",
        "#### üîπ Why RBF Works Well:\n",
        "\n",
        "* It considers **distance between points**: closer points are more similar.\n",
        "* Allows the SVM to create a flexible, curved decision boundary.\n",
        "\n",
        "\n",
        "\n",
        "### Visual Analogy:\n",
        "\n",
        "* Imagine trying to separate two classes shaped like concentric circles.\n",
        "* In 2D, it's impossible with a straight line.\n",
        "* The kernel trick (e.g., RBF) maps the data into a higher-dimensional space where the two circles become **separable by a plane**.\n",
        "\n",
        "\n",
        "# Question 4: What is a Na√Øve Bayes Classifier, and why is it called ‚Äúna√Øve‚Äù?\n",
        "\n",
        "# Answer:\n",
        "\n",
        "###  What is a Na√Øve Bayes Classifier?\n",
        "\n",
        "A **Na√Øve Bayes Classifier** is a **supervised learning algorithm** based on **Bayes' Theorem**, used primarily for **classification tasks**. It is especially effective in **text classification**, **spam detection**, and **sentiment analysis**.\n",
        "\n",
        "\n",
        "\n",
        "###  How It Works:\n",
        "\n",
        "It calculates the **probability of each class** given a set of input features and assigns the class with the **highest probability**.\n",
        "\n",
        "#### üîπ Bayes' Theorem:\n",
        "\n",
        "$$\n",
        "P(C|X) = \\frac{P(X|C) \\cdot P(C)}{P(X)}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $P(C|X)$: Posterior probability of class $C$ given features $X$\n",
        "* $P(X|C)$: Likelihood of features given class\n",
        "* $P(C)$: Prior probability of class\n",
        "* $P(X)$: Evidence (constant for all classes)\n",
        "\n",
        "\n",
        "\n",
        "###  Why Is It Called **‚ÄúNa√Øve‚Äù**?\n",
        "\n",
        "It is called **na√Øve** because it **assumes that all features are independent** of each other **given the class label**.\n",
        "\n",
        "#### Example:\n",
        "\n",
        "* In spam classification, Na√Øve Bayes assumes that the presence of the word ‚Äúfree‚Äù is independent of the presence of the word ‚Äúoffer,‚Äù even though in real life, they often appear together in spam emails.\n",
        "\n",
        "This **strong independence assumption** is often **not true**, which is why the algorithm is called \"na√Øve.\"\n",
        "\n",
        "\n",
        "### üîπ Despite the ‚ÄúNa√Øve‚Äù Assumption, It Works Well:\n",
        "\n",
        "* Especially in high-dimensional problems like **text classification**, where the independence assumption is often ‚Äúgood enough.‚Äù\n",
        "* It's also **fast**, **scalable**, and performs surprisingly well even when the independence assumption is violated.\n",
        "\n",
        "\n",
        "\n",
        "### Types of Na√Øve Bayes Classifiers:\n",
        "\n",
        "| Type               | Use Case                                          |\n",
        "| ------------------ | ------------------------------------------------- |\n",
        "| **Multinomial NB** | Text classification (word counts)                 |\n",
        "| **Bernoulli NB**   | Binary/Boolean features (e.g., word presence)     |\n",
        "| **Gaussian NB**    | Continuous features (assumes normal distribution) |\n",
        "\n",
        "\n",
        "\n",
        "### üîπ Example Use Case:\n",
        "\n",
        "**Spam Detection**: Given features like word frequency, Na√Øve Bayes calculates the probability of an email being spam and classifies it accordingly.\n",
        "\n",
        "# Question 5: Describe the Gaussian, Multinomial, and Bernoulli Na√Øve Bayes variants. When would you use each one?\n",
        "\n",
        "# Answer:\n",
        "###  1. **Gaussian Na√Øve Bayes**\n",
        "\n",
        "####  Use when:\n",
        "\n",
        "* Your features are **continuous numerical values** (e.g., height, weight, temperature).\n",
        "* The features are assumed to follow a **normal (Gaussian) distribution**.\n",
        "\n",
        "####  How it works:\n",
        "\n",
        "* It models the likelihood of features using the **Gaussian (bell curve) distribution**.\n",
        "* For each feature, it calculates the **mean** and **variance** from the training data to compute the probability.\n",
        "\n",
        "####  Use case examples:\n",
        "\n",
        "* Medical data (e.g., diagnosing based on lab results)\n",
        "* Sensor data\n",
        "* Iris dataset (flower classification)\n",
        "\n",
        "\n",
        "\n",
        "###  2. **Multinomial Na√Øve Bayes**\n",
        "\n",
        "####  Use when:\n",
        "\n",
        "* Your features are **discrete counts** (e.g., word counts in text).\n",
        "* Often used in **Natural Language Processing (NLP)** tasks.\n",
        "\n",
        "####  How it works:\n",
        "\n",
        "* Assumes that features represent **frequencies or counts** (e.g., how many times a word appears in a document).\n",
        "* Calculates the probability of features occurring in a given class.\n",
        "\n",
        "#### Use case examples:\n",
        "\n",
        "* Text classification (e.g., spam detection, topic labeling)\n",
        "* Document categorization\n",
        "* Sentiment analysis (based on word frequency)\n",
        "\n",
        "\n",
        "\n",
        "### 3. **Bernoulli Na√Øve Bayes**\n",
        "\n",
        "####  Use when:\n",
        "\n",
        "* Your features are **binary (0 or 1)** ‚Äî i.e., feature is **present or absent**.\n",
        "* Still useful in **text classification**, but using presence/absence of words instead of their frequency.\n",
        "\n",
        "####  How it works:\n",
        "\n",
        "* Assumes each feature follows a **Bernoulli (binary) distribution**.\n",
        "* Calculates the probability that a feature is present or absent given the class.\n",
        "\n",
        "#### Use case examples:\n",
        "\n",
        "* Spam filtering (based on whether a word appears or not)\n",
        "* Classifying emails or tweets by presence of certain keywords\n",
        "\n"
      ],
      "metadata": {
        "id": "B1TPhdYIvYYM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SVM & Naive Bayes Assignment Practical Questions"
      ],
      "metadata": {
        "id": "OuJFxjrH2aui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6:   Write a Python program to: ‚óè Load the Iris dataset ‚óè Train an SVM Classifier with a linear kernel ‚óè Print the model's accuracy and support vectors.\n",
        "\n",
        "# Answer:\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data       # Feature matrix\n",
        "y = iris.target     # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Create and train the SVM model with a linear kernel\n",
        "svm_model = SVC(kernel='linear')\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "# Step 5: Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Print support vectors\n",
        "print(\"\\nSupport Vectors:\")\n",
        "print(svm_model.support_vectors_)\n",
        "\n",
        "# Optional: Print support vector indices and counts per class\n",
        "print(\"\\nIndices of Support Vectors:\")\n",
        "print(svm_model.support_)\n",
        "\n",
        "print(\"\\nNumber of Support Vectors for Each Class:\")\n",
        "print(svm_model.n_support_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJHhmhCWxqEJ",
        "outputId": "048f2417-4009-4987-94a7-0f2ba48b8a5c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n",
            "\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n",
            "\n",
            "Indices of Support Vectors:\n",
            "[ 31  33  91  22  45  54  59  60  62  73  79  80 105 110   5  16  30  42\n",
            "  68  81  87 101 112 113 116]\n",
            "\n",
            "Number of Support Vectors for Each Class:\n",
            "[ 3 11 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7:  Write a Python program to: ‚óè Load the Breast Cancer dataset ‚óè Train a Gaussian Na√Øve Bayes model ‚óè Print its classification report including precision, recall, and F1-score.\n",
        "\n",
        "# Answer:\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data         # Features\n",
        "y = data.target       # Labels (0 = malignant, 1 = benign)\n",
        "\n",
        "# Step 2: Split the dataset into training and test sets (80/20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Train a Gaussian Na√Øve Bayes model\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print the classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ucrTwTqx_g7",
        "outputId": "e7cf2ac1-ec9c-4db3-f5a8-2d30096301cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant       1.00      0.93      0.96        43\n",
            "      benign       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to: ‚óè Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma. ‚óè Print the best hyperparameters and accuracy.\n",
        "\n",
        "# Answer:\n",
        "# Import necessary libraries\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data       # Feature matrix\n",
        "y = wine.target     # Target labels\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Define the SVM model\n",
        "svm_model = SVC()\n",
        "\n",
        "# Step 4: Define the parameter grid for C and gamma\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],           # Regularization parameter\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],   # Kernel coefficient\n",
        "    'kernel': ['rbf']                # Using RBF kernel\n",
        "}\n",
        "\n",
        "# Step 5: Use GridSearchCV to find the best parameters\n",
        "grid_search = GridSearchCV(svm_model, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print results\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "print(f\"\\nTest Set Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oIQ8md5yhxO",
        "outputId": "46aafa4b-9397-4799-8b19-4a6d4a0b0118"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "\n",
            "Test Set Accuracy: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to: ‚óè Train a Na√Øve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups). ‚óè Print the model's ROC-AUC score for its predictions.\n",
        "\n",
        "# Answer:\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Step 1: Load a binary subset of the 20 Newsgroups dataset\n",
        "categories = ['sci.med', 'soc.religion.christian']  # Binary classification\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "# Step 2: Convert text to TF-IDF features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(data.data)\n",
        "y = data.target  # Binary labels (0 or 1)\n",
        "\n",
        "# Step 3: Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Train a Multinomial Na√Øve Bayes classifier\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict probabilities and compute ROC-AUC\n",
        "y_probs = model.predict_proba(X_test)[:, 1]  # Probabilities for class 1\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "\n",
        "# Step 6: Print ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bop6aTSty69X",
        "outputId": "269eee04-e9e8-434f-b19f-e56b9ab3c547"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9932\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Question 10: Imagine you‚Äôre working as a data scientist for a company that handles email communications. Your task is to automatically classify emails as Spam or Not Spam. The emails may contain: ‚óè Text with diverse vocabulary ‚óè Potential class imbalance (far more legitimate emails than spam) ‚óè Some incomplete or missing data Explain the approach you would take to: ‚óè Preprocess the data (e.g. text vectorization, handling missing data) ‚óè Choose and justify an appropriate model (SVM vs. Na√Øve Bayes) ‚óè Address class imbalance ‚óè Evaluate the performance of your solution with suitable metrics And explain the business impact of your solution.\n",
        "\n",
        "#  Answer:\n",
        "\n",
        "### 1. **Preprocessing the Data**\n",
        "\n",
        "* **Text Cleaning & Normalization:**\n",
        "\n",
        "  * Remove special characters, punctuation, and HTML tags.\n",
        "  * Convert text to lowercase to ensure uniformity.\n",
        "  * Optionally apply stemming or lemmatization to reduce words to their base form.\n",
        "\n",
        "* **Handling Missing or Incomplete Data:**\n",
        "\n",
        "  * Identify missing values (e.g., empty emails or missing fields).\n",
        "  * Impute missing values where possible or discard incomplete entries if they are few.\n",
        "  * For text data, empty or very short emails could be flagged or handled separately.\n",
        "\n",
        "* **Text Vectorization:**\n",
        "\n",
        "  * Use **TF-IDF Vectorizer** or **Count Vectorizer** to convert text into numerical feature vectors.\n",
        "  * Consider n-grams (unigrams + bigrams) to capture context and phrases.\n",
        "  * Possibly use dimensionality reduction (e.g., TruncatedSVD) if the feature space is too large.\n",
        "\n",
        "\n",
        "\n",
        "### 2. **Choosing and Justifying an Appropriate Model**\n",
        "\n",
        "| Model           | Pros                                                                     | Cons                                                      | Suitability for Spam Classification                             |\n",
        "| --------------- | ------------------------------------------------------------------------ | --------------------------------------------------------- | --------------------------------------------------------------- |\n",
        "| **Na√Øve Bayes** | Fast, simple, works well with text, handles high-dimensional sparse data | Assumes feature independence (na√Øve assumption)           | Excellent baseline; commonly used in spam filters               |\n",
        "| **SVM**         | Effective with high-dimensional data, flexible with kernels              | Can be slower to train on large datasets; tuning required | Powerful if well-tuned, better with complex decision boundaries |\n",
        "\n",
        "* **Recommended Approach:**\n",
        "\n",
        "  * Start with **Multinomial Na√Øve Bayes** because it is fast, handles text data natively, and performs well even with diverse vocabulary.\n",
        "  * If Na√Øve Bayes accuracy is insufficient, try **SVM with a linear or RBF kernel**, tuning hyperparameters via GridSearchCV.\n",
        "  * Use cross-validation to compare model performance.\n",
        "\n",
        "\n",
        "\n",
        "### 3. **Addressing Class Imbalance**\n",
        "\n",
        "* Since **spam emails are often much fewer** than legitimate ones, imbalance can bias the model toward the majority class.\n",
        "\n",
        "**Strategies:**\n",
        "\n",
        "* **Resampling Techniques:**\n",
        "\n",
        "  * **Oversampling** minority class (e.g., SMOTE).\n",
        "  * **Undersampling** majority class.\n",
        "\n",
        "* **Class Weighting:**\n",
        "\n",
        "  * Use model parameters to assign higher weight to the minority class (e.g., `class_weight='balanced'` in SVM).\n",
        "\n",
        "* **Threshold Tuning:**\n",
        "\n",
        "  * Adjust classification threshold based on precision-recall trade-offs rather than default 0.5.\n",
        "\n",
        "* **Ensemble Methods:**\n",
        "\n",
        "  * Combine multiple models to improve minority class detection.\n",
        "\n",
        "\n",
        "\n",
        "### 4. **Evaluating Performance with Suitable Metrics**\n",
        "\n",
        "* **Accuracy** alone is misleading in imbalanced datasets (e.g., predicting all emails as non-spam could yield high accuracy).\n",
        "\n",
        "**Better Metrics:**\n",
        "\n",
        "* **Precision:** How many predicted spam emails are actually spam? (Important to avoid false positives)\n",
        "\n",
        "* **Recall (Sensitivity):** How many actual spam emails were correctly detected? (Important to catch spam)\n",
        "\n",
        "* **F1-Score:** Harmonic mean of precision and recall, balances both.\n",
        "\n",
        "* **ROC-AUC:** Measures overall ability to discriminate between classes.\n",
        "\n",
        "* **Precision-Recall Curve:** More informative than ROC for imbalanced data.\n",
        "\n",
        "* Also, monitor **False Positive Rate (FPR)** carefully, as wrongly classifying legitimate emails as spam can hurt user trust.\n",
        "\n",
        "\n",
        "\n",
        "### 5. **Business Impact**\n",
        "\n",
        "* **Automating spam detection** reduces manual effort and improves user experience by filtering unwanted emails.\n",
        "* **Accurate spam filtering** protects users from phishing, scams, and malware, improving security.\n",
        "* **Minimizing false positives** ensures legitimate emails aren‚Äôt lost or delayed, maintaining customer satisfaction and communication reliability.\n",
        "* **Adaptability** through continuous retraining helps cope with evolving spam tactics, reducing long-term risk.\n",
        "* **Operational efficiency** improves by freeing IT resources and reducing storage/bandwidth from spam emails.\n",
        "\n"
      ],
      "metadata": {
        "id": "kmxVfmSFzWah"
      }
    }
  ]
}
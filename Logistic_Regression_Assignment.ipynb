{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3iPsXvNUGQa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 1: What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "#Ans:\n",
        "**Logistic Regression** and **Linear Regression** are both supervised machine learning algorithms used for prediction, but they are designed for **different types of problems** and have **different output formats**.\n",
        "\n",
        "\n",
        "\n",
        "### ðŸ”¹ **Logistic Regression**\n",
        "\n",
        "**Purpose**: Used for **classification problems**, especially **binary classification** (e.g., spam or not spam, diseased or healthy).\n",
        "\n",
        "**Output**: Predicts the **probability** that a given input belongs to a particular class. The final output is a value between **0 and 1**.\n",
        "\n",
        "**How It Works**:\n",
        "\n",
        "* It uses a **sigmoid (logistic) function** to map the linear combination of input features to a probability:\n",
        "\n",
        "  $$\n",
        "  P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_nx_n)}}\n",
        "  $$\n",
        "* If the probability is > 0.5, classify as **1**, otherwise as **0** (this threshold can be adjusted).\n",
        "\n",
        "\n",
        "\n",
        "### ðŸ”¹ **Linear Regression**\n",
        "\n",
        "**Purpose**: Used for **regression problems**, where the output is a **continuous value** (e.g., predicting house prices, temperature, etc.).\n",
        "\n",
        "**Output**: Directly predicts a **numeric value** without bounding it.\n",
        "\n",
        "**How It Works**:\n",
        "\n",
        "* Uses a linear equation to model the relationship:\n",
        "\n",
        "  $$\n",
        "  y = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_nx_n\n",
        "  $$\n",
        "\n",
        "\n",
        "\n",
        "###  **Key Differences**\n",
        "\n",
        "| Feature                 | Logistic Regression           | Linear Regression        |\n",
        "| ----------------------- | ----------------------------- | ------------------------ |\n",
        "| **Type of Problem**     | Classification (binary/multi) | Regression               |\n",
        "| **Output Range**        | 0 to 1 (probability)          | $-\\infty$ to $+\\infty$   |\n",
        "| **Activation Function** | Sigmoid function              | None (purely linear)     |\n",
        "| **Prediction**          | Class label (e.g., 0 or 1)    | Numeric value            |\n",
        "| **Loss Function**       | Log Loss (Cross-Entropy)      | Mean Squared Error (MSE) |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Question 2: Explain the role of the Sigmoid function in Logistic Regression.\n",
        "\n",
        "#Ans:\n",
        "\n",
        "\n",
        "### ðŸ”¹ What is the Sigmoid Function?\n",
        "\n",
        "The **sigmoid function** is a mathematical function that maps any real-valued number into a value **between 0 and 1**. It is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_nx_n$ (i.e., the **linear combination** of input features)\n",
        "\n",
        "\n",
        "\n",
        "### ðŸ”¹ Role of the Sigmoid Function in Logistic Regression\n",
        "\n",
        "In **Logistic Regression**, the sigmoid function is used to:\n",
        "\n",
        "#### 1. **Convert Linear Output to Probability**\n",
        "\n",
        "* Logistic Regression first calculates a **linear score** (like in Linear Regression).\n",
        "* The **sigmoid function** is then applied to this score to **squash** the output to a range between **0 and 1**, making it interpretable as a **probability**.\n",
        "\n",
        "$$\n",
        "P(y=1 \\mid x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1x_1 + \\ldots + \\beta_nx_n)}}\n",
        "$$\n",
        "\n",
        "#### 2. **Enable Binary Classification**\n",
        "\n",
        "* Once the probability is computed, a **threshold** (commonly 0.5) is used to classify:\n",
        "\n",
        "  * If probability â‰¥ 0.5 â†’ predict class **1**\n",
        "  * If probability < 0.5 â†’ predict class **0**\n",
        "\n",
        "#### 3. **Smooth Gradient for Optimization**\n",
        "\n",
        "* The sigmoid function is **differentiable**, which is crucial for using **gradient descent** to optimize the logistic regression model during training.\n",
        "\n",
        "\n",
        "\n",
        "### ðŸ”¸ Visual Understanding\n",
        "\n",
        "The sigmoid function curve looks like this:\n",
        "\n",
        "```\n",
        "   1 |                          ***\n",
        "     |                       ***\n",
        "     |                    ***\n",
        "     |                 ***\n",
        "     |              ***\n",
        "     |          ****\n",
        "     |     ****\n",
        "     |  ***\n",
        "   0 +-------------------------------\n",
        "      -6   -3    0    3    6\n",
        "```\n",
        "\n",
        "* As $z \\to +\\infty$, sigmoid â†’ 1\n",
        "* As $z \\to -\\infty$, sigmoid â†’ 0\n",
        "* At $z = 0$, sigmoid = 0.5\n",
        "\n",
        "\n",
        "#Question 3: What is Regularization in Logistic Regression and why is it needed ?\n",
        "\n",
        "#Ans:\n",
        "\n",
        "\n",
        "### ðŸ”¹ What is Regularization?\n",
        "\n",
        "**Regularization** is a technique used to **prevent overfitting** in machine learning models â€” including **logistic regression** â€” by **penalizing large coefficients** in the model.\n",
        "\n",
        "In logistic regression, regularization modifies the **loss function** by adding a **penalty term** based on the magnitude of the model's coefficients.\n",
        "\n",
        "\n",
        "### ðŸ”¹ Why Is Regularization Needed?\n",
        "\n",
        "Without regularization:\n",
        "\n",
        "* The model might learn to **fit the training data too well**, especially if there are many features.\n",
        "* This can lead to **overfitting**, where the model performs well on training data but poorly on unseen (test) data.\n",
        "* Overfitting often occurs when the model assigns **very large weights** to certain features, making it sensitive to noise.\n",
        "\n",
        "\n",
        "### ðŸ”¹ Types of Regularization in Logistic Regression\n",
        "\n",
        "#### 1. **L1 Regularization (Lasso)**\n",
        "\n",
        "* Adds the **sum of the absolute values** of the coefficients to the loss function.\n",
        "* Formula:\n",
        "\n",
        "  $$\n",
        "  Loss = \\text{Log Loss} + \\lambda \\sum_{j=1}^{n} |\\beta_j|\n",
        "  $$\n",
        "* Encourages **sparsity** (i.e., drives some coefficients to zero), which helps in **feature selection**.\n",
        "\n",
        "#### 2. **L2 Regularization (Ridge)**\n",
        "\n",
        "* Adds the **sum of the squares** of the coefficients to the loss function.\n",
        "* Formula:\n",
        "\n",
        "  $$\n",
        "  Loss = \\text{Log Loss} + \\lambda \\sum_{j=1}^{n} \\beta_j^2\n",
        "  $$\n",
        "* Encourages **smaller weights** but doesnâ€™t force them to zero. Helps in controlling **model complexity**.\n",
        "\n",
        ">  **Note**: $\\lambda$ (also called the **regularization parameter**) controls the **strength of the penalty**. A larger $\\lambda$ means **more regularization**.\n",
        "\n",
        "\n",
        "\n",
        "### ðŸ”¸ Regularized Logistic Regression Loss Function (Example with L2):\n",
        "\n",
        "$$\n",
        "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[y^{(i)} \\log(h_\\theta(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_\\theta(x^{(i)}))\\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "\n",
        "#Question 4: What are some common evaluation metrics for classification models, and why are they important?\n",
        "\n",
        "#Ans:\n",
        "\n",
        "\n",
        "### ðŸ”¹ Why Are Evaluation Metrics Important?\n",
        "\n",
        "* They **quantify model performance**, helping you understand **how well** your classification model is doing.\n",
        "* Different problems require different metrics â€” especially in **imbalanced datasets**, where accuracy can be misleading.\n",
        "* The **choice of metric** impacts model selection, tuning, and real-world deployment decisions.\n",
        "\n",
        "\n",
        "\n",
        "### ðŸ”¹ Common Evaluation Metrics for Classification\n",
        "\n",
        "| Metric | Description |\n",
        "| ------ | ----------- |\n",
        "\n",
        "\n",
        "\n",
        "#### 1. **Accuracy**\n",
        "\n",
        "* **Definition**: The ratio of correctly predicted instances to total instances.\n",
        "\n",
        "  $$\n",
        "  \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "  $$\n",
        "* **Use Case**: Good when the classes are **balanced**.\n",
        "\n",
        ">  **Limitation**: Misleading when classes are imbalanced (e.g., 95% of one class).\n",
        "\n",
        "\n",
        "#### 2. **Precision**\n",
        "\n",
        "* **Definition**: Proportion of correctly predicted positive cases out of all predicted positives.\n",
        "\n",
        "  $$\n",
        "  \\text{Precision} = \\frac{TP}{TP + FP}\n",
        "  $$\n",
        "* **Use Case**: Important when **false positives** are costly (e.g., spam filter).\n",
        "\n",
        "#### 3. **Recall (Sensitivity / True Positive Rate)**\n",
        "\n",
        "* **Definition**: Proportion of correctly predicted positive cases out of all actual positives.\n",
        "\n",
        "  $$\n",
        "  \\text{Recall} = \\frac{TP}{TP + FN}\n",
        "  $$\n",
        "* **Use Case**: Important when **false negatives** are costly (e.g., disease detection).\n",
        "\n",
        "\n",
        "\n",
        "#### 4. **F1 Score**\n",
        "\n",
        "* **Definition**: Harmonic mean of precision and recall.\n",
        "\n",
        "  $$\n",
        "  F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "  $$\n",
        "* **Use Case**: Useful when thereâ€™s **class imbalance** and you need a **balance** between precision and recall.\n",
        "\n",
        "\n",
        "\n",
        "#### 5. **Confusion Matrix**\n",
        "\n",
        "* A 2x2 table for binary classification that shows:\n",
        "\n",
        "  * **True Positives (TP)**, **True Negatives (TN)**\n",
        "  * **False Positives (FP)**, **False Negatives (FN)**\n",
        "* Helps visualize **where** the model is making mistakes.\n",
        "\n",
        "\n",
        "\n",
        "#### 6. **ROC Curve & AUC (Area Under Curve)**\n",
        "\n",
        "* **ROC Curve**: Plots **True Positive Rate vs. False Positive Rate**.\n",
        "* **AUC**: Measures the area under the ROC curve (value between 0 and 1).\n",
        "\n",
        "  * Closer to 1 = better classifier.\n",
        "\n",
        ">  Use **AUC-ROC** when you want to measure how well the model ranks predictions across thresholds.\n",
        "\n",
        "\n",
        "\n",
        "#### 7. **Log Loss (Cross-Entropy Loss)**\n",
        "\n",
        "* Measures the uncertainty in your predicted probabilities.\n",
        "* **Penalty** is high for confident wrong predictions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Wpc9Dx-gUuko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 5: Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy.(Use Dataset from sklearn package)\n",
        "\n",
        "# Ans:\n",
        "\n",
        "'''Here's a complete Python program that uses the **`sklearn`** library to:\n",
        "\n",
        "1. Load a dataset (e.g., **Breast Cancer** dataset from `sklearn.datasets`)\n",
        "2. Convert it into a **Pandas DataFrame**\n",
        "3. Split the data into **training and test sets**\n",
        "4. Train a **Logistic Regression** model\n",
        "5. Print the **accuracy** of the model\n",
        "\n",
        "```python'''\n",
        "\n",
        "# Step 1: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Step 3: Create a DataFrame from the dataset\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target  # Add target column\n",
        "\n",
        "# Step 4: Split into features (X) and target (y)\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Step 5: Split into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Create and train the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=10000)  # Use high max_iter to ensure convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Accuracy: {accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVHDGk8xWG_s",
        "outputId": "85781809-48fa-4882-cd4b-1ff5fa18c5ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 6: Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy.\n",
        "\n",
        "# Ans:\n",
        "\n",
        "\n",
        "\n",
        "'''* Loads a dataset from `sklearn` (weâ€™ll use the **Iris** dataset for variety),\n",
        "* Trains a **Logistic Regression model with L2 regularization** (Ridge),\n",
        "* Prints the **model coefficients** and **accuracy**.\n",
        "\n",
        "```python'''\n",
        "# Step 1: Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Step 3: Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Step 4: Prepare features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Step 5: Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression with L2 regularization (default)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', multi_class='auto', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Print model coefficients\n",
        "print(\"Model Coefficients (per class):\")\n",
        "for idx, class_label in enumerate(model.classes_):\n",
        "    print(f\"Class {class_label}: {model.coef_[idx]}\")\n",
        "\n",
        "# Step 9: Print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-IZUYqbWlh7",
        "outputId": "e15d4ae7-f45d-425b-e3db-33cf788b8fb5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients (per class):\n",
            "Class 0: [-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            "Class 1: [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            "Class 2: [-0.11497673 -0.70769055  2.58813565  1.7744936 ]\n",
            "\n",
            "Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report.\n",
        "\n",
        "# Ans:\n",
        "\n",
        "'''\n",
        "* Load a **multiclass dataset** (we'll use the **Iris** dataset from `sklearn`)\n",
        "* Train a **Logistic Regression model** using `multi_class='ovr'` (One-vs-Rest strategy)\n",
        "* Print the **classification report** with precision, recall, F1-score, and support\n",
        "\n",
        "```python'''\n",
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Step 2: Load the Iris dataset\n",
        "data = load_iris()\n",
        "\n",
        "# Step 3: Convert to DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Step 4: Features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Step 5: Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression with One-vs-Rest strategy\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 8: Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=data.target_names))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMBJmmDrXTnd",
        "outputId": "533b0da9-dbd6-4b2e-e5ac-91b36e4ad485"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy.\n",
        "\n",
        "# Ans:\n",
        "\n",
        "'''\n",
        "* Loads a dataset (weâ€™ll use the **Wine dataset** from `sklearn.datasets`)\n",
        "* Uses **`LogisticRegression`**\n",
        "* Applies **`GridSearchCV`** to tune:\n",
        "\n",
        "  * `C`: Inverse of regularization strength\n",
        "  * `penalty`: Type of regularization (`l1`, `l2`)\n",
        "* Prints the **best parameters** and **best cross-validated score**\n",
        "\n",
        "```python'''\n",
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load the Wine dataset\n",
        "data = load_wine()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Step 3: Split into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports both 'l1' and 'l2'\n",
        "}\n",
        "\n",
        "# Step 5: Create GridSearchCV with Logistic Regression\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=LogisticRegression(max_iter=1000),\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Step 6: Fit the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Get best parameters and accuracy\n",
        "best_model = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Step 8: Evaluate on test set\n",
        "test_accuracy = accuracy_score(y_test, best_model.predict(X_test))\n",
        "\n",
        "# Step 9: Print results\n",
        "print(\"Best Hyperparameters:\")\n",
        "print(best_params)\n",
        "\n",
        "print(f\"\\nBest Cross-Validation Accuracy: {best_score:.4f}\")\n",
        "print(f\"Test Set Accuracy: {test_accuracy:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRyepjBXXzgj",
        "outputId": "08165bc8-f356-4933-ed10-64eb6fea02f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters:\n",
            "{'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
            "\n",
            "Best Cross-Validation Accuracy: 0.9507\n",
            "Test Set Accuracy: 0.9722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.\n",
        "\n",
        "# Ans:\n",
        "'''\n",
        "* Training a **Logistic Regression** model on raw features\n",
        "* Training the **same model** on **standardized (scaled)** features\n",
        "* Comparing the **accuracy** of both approaches\n",
        "\n",
        "We'll use the **Breast Cancer dataset** from `sklearn.datasets` â€” it's a binary classification problem with features of varying scales.\n",
        "\n",
        "```python'''\n",
        "# Step 1: Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 2: Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "# Step 3: Split into training and testing sets\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ----------- Without Feature Scaling -----------\n",
        "# Step 4a: Train Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=10000)\n",
        "model_raw.fit(X_train_raw, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test_raw)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# ----------- With Feature Scaling -------------\n",
        "# Step 4b: Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
        "X_test_scaled = scaler.transform(X_test_raw)\n",
        "\n",
        "# Step 5: Train Logistic Regression on scaled data\n",
        "model_scaled = LogisticRegression(max_iter=10000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# ----------- Compare Results ------------------\n",
        "print(f\"Accuracy WITHOUT Scaling: {accuracy_raw:.4f}\")\n",
        "print(f\"Accuracy WITH Scaling   : {accuracy_scaled:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_GzogkcYhJN",
        "outputId": "a02ca471-e30c-495e-bd90-d7edda487a58"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.9561\n",
            "Accuracy WITH Scaling   : 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 10: Imagine you are working at an e-commerce company that wants to predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach youâ€™d take to build a Logistic Regression model â€” including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.\n",
        "\n",
        "# Answer:\n",
        "\n",
        "\n",
        "###  **Goal**: Predict which customers are likely to respond to a marketing campaign\n",
        "\n",
        "(Only 5% positive class â€” **imbalanced binary classification**)\n",
        "\n",
        "\n",
        "\n",
        "## ðŸ”· Step-by-Step Approach to Building the Logistic Regression Model\n",
        "\n",
        "\n",
        "\n",
        "### **1. Data Understanding & Exploration**\n",
        "\n",
        "* **Check Class Distribution**: Confirm the 5% response rate.\n",
        "* **Explore Features**: Understand categorical vs. numerical features.\n",
        "* **Detect Missing Data**, outliers, correlations, etc.\n",
        "* Perform **EDA** (Exploratory Data Analysis) to identify strong predictors.\n",
        "\n",
        "\n",
        "\n",
        "### **2. Data Preprocessing**\n",
        "\n",
        "#### ðŸ”¹ **Feature Engineering**\n",
        "\n",
        "* Create or transform features that may capture behavior:\n",
        "\n",
        "  * Recency, frequency, monetary value (RFM features)\n",
        "  * Previous campaign responses\n",
        "  * Demographics, purchase history\n",
        "\n",
        "#### ðŸ”¹ **Handle Missing Values**\n",
        "\n",
        "* Impute using mean/median (for numeric), mode (for categorical), or drop if too sparse.\n",
        "\n",
        "#### ðŸ”¹ **Encoding**\n",
        "\n",
        "* Use **one-hot encoding** for nominal categorical variables.\n",
        "* Use **label encoding** for ordinal features if needed.\n",
        "\n",
        "#### ðŸ”¹ **Feature Scaling**\n",
        "\n",
        "* Apply **StandardScaler** to numerical features before training:\n",
        "\n",
        "  * Logistic Regression is sensitive to scale since it uses optimization algorithms.\n",
        "\n",
        "\n",
        "\n",
        "### **3. Addressing Class Imbalance**\n",
        "\n",
        "Since only 5% of customers respond, **class imbalance** must be handled carefully.\n",
        "\n",
        "#### Options:\n",
        "\n",
        "* **Resampling**:\n",
        "\n",
        "  * **Oversample** the minority class using:\n",
        "\n",
        "    * `SMOTE` (Synthetic Minority Oversampling Technique)\n",
        "    * Random oversampling\n",
        "  * **Undersample** the majority class\n",
        "* **Use class weights**:\n",
        "\n",
        "  * Set `class_weight='balanced'` in `LogisticRegression` to penalize misclassifying the minority class more heavily.\n",
        "\n",
        "\n",
        "\n",
        "### **4. Train/Test Split**\n",
        "\n",
        "* Use `train_test_split(stratify=y)` to maintain class balance in both sets.\n",
        "* Alternatively, use **stratified cross-validation** during tuning.\n",
        "\n",
        "\n",
        "\n",
        "### **5. Train the Logistic Regression Model**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(\n",
        "    class_weight='balanced',  # handles imbalance\n",
        "    solver='liblinear',       # robust for small datasets and supports L1\n",
        "    max_iter=1000\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **6. Hyperparameter Tuning (GridSearchCV)**\n",
        "\n",
        "Tune:\n",
        "\n",
        "* `C`: Regularization strength\n",
        "* `penalty`: L1 or L2\n",
        "* Possibly `class_weight` if not using 'balanced'\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(LogisticRegression(class_weight='balanced'), param_grid, cv=5, scoring='f1')\n",
        "grid.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "### **7. Model Evaluation â€” Focus on the Right Metrics**\n",
        "\n",
        "**Accuracy is misleading in imbalanced problems. Instead, use:**\n",
        "\n",
        "| Metric               | Why It's Important                                                   |\n",
        "| -------------------- | -------------------------------------------------------------------- |\n",
        "| **Precision**        | High precision = less false positives (don't waste marketing budget) |\n",
        "| **Recall**           | High recall = find more true responders (maximize ROI)               |\n",
        "| **F1 Score**         | Balance between precision and recall                                 |\n",
        "| **ROC AUC**          | Measures modelâ€™s ability to rank responders higher                   |\n",
        "| **PR AUC**           | Especially useful when the positive class is rare                    |\n",
        "| **Confusion Matrix** | Understand types of errors made                                      |\n",
        "\n",
        "\n",
        "\n",
        "### **8. Post-Modeling Business Considerations**\n",
        "\n",
        "* **Threshold Tuning**:\n",
        "\n",
        "  * Default threshold (0.5) might not be optimal â€” try lowering it to increase recall.\n",
        "  * Use `precision-recall curve` to choose the best probability threshold for business goals.\n",
        "\n",
        "* **Lift Chart / Gain Chart**:\n",
        "\n",
        "  * Evaluate how many responders are captured in top X% of predicted probabilities.\n",
        "\n",
        "* **Profit Analysis**:\n",
        "\n",
        "  * Model should be optimized based on **expected profit/loss**, not just accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "OdRLPUNaZPgX"
      }
    }
  ]
}